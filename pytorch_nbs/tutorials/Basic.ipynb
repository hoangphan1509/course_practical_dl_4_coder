{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "# Build a computational graph\n",
    "y = w * x + b\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out the gradients.\n",
    "print(x.grad.data)\n",
    "print(w.grad.data)\n",
    "print(b.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for linear model y = w * x +  b\n",
    "x = Variable(torch.rand(30, 2))\n",
    "w = Variable(torch.Tensor([2, 3]).view(2, -1))\n",
    "y = torch.mm(x, w) + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.6921\n",
       " 3.8060\n",
       " 3.7515\n",
       " 4.8494\n",
       " 5.3382\n",
       "[torch.FloatTensor of size 5x1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  \n",
      " 0.5040 -0.4051\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "b:  \n",
      " 0.6890\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(2, 1)\n",
    "print('w: ', linear.weight.data)\n",
    "print('b: ',linear.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "pred = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.751127243041992\n"
     ]
    }
   ],
   "source": [
    "# compute loss\n",
    "loss = criterion(pred, y)\n",
    "print('loss:', loss.data[0])\n",
    "\n",
    "# backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw Variable containing:\n",
      "-3.0015 -3.3085\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "dL/db Variable containing:\n",
      "-5.6157\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradients\n",
    "print('dL/dw', linear.weight.grad)\n",
    "print('dL/db', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 step optimization:  8.244101524353027\n",
      "w:  \n",
      " 0.5340 -0.3720\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "b:  \n",
      " 0.7452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "optimizer.step()\n",
    "\n",
    "# Print out the loss after optimization.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.data[0])\n",
    "linear.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print('w: ', linear.weight.data)\n",
    "print('b: ', linear.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download and construct dataset\n",
    "train_dataset = dsets.CIFAR10(root='../../data/', \n",
    "                              train=True, \n",
    "                              transform=transforms.ToTensor(), \n",
    "                              download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('plane', 'car', 'bird', 'cat',\n",
    "'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAACHCAYAAAAC53JtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngnFiNzMrmP8HHZUzP96DEHMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8500e16400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img, title=None, mean=0, std=1):\n",
    "    npimg = img.numpy().transpose((1, 2, 0))\n",
    "    npimg = std * npimg + mean\n",
    "    npimg = np.clip(npimg, 0, 1)\n",
    "    plt.imshow(npimg)\n",
    "                                  \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "                                  \n",
    "out = torchvision.utils.make_grid(images)\n",
    "                                  \n",
    "imshow(out, title=[class_names[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=10,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)\n",
    "\n",
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = data_iter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "num_epoches = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = dsets.MNIST(root='../../data/', \n",
    "                            train=True, \n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "test_dataset = dsets.MNIST(root='../../data/', \n",
    "                           train=False,\n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[4 x 1]' is invalid for input of with 100 elements at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/TH/THStorage.c:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-f141def340e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: size '[4 x 1]' is invalid for input of with 100 elements at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/TH/THStorage.c:41"
     ]
    }
   ],
   "source": [
    "print(labels.view(4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9\n",
      " 5\n",
      " 3\n",
      " 9\n",
      "[torch.LongTensor of size 4x1]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "    0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     1     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     1\n",
      "[torch.FloatTensor of size 4x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "nb_digits = 10\n",
    "\n",
    "# Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "y = torch.LongTensor(batch_size,1).random_() % nb_digits\n",
    "\n",
    "# One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
    "\n",
    "# In your for loop\n",
    "y_onehot.zero_()\n",
    "y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "print(y)\n",
    "print(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     1     0     0\n",
       "    0     0     0     0     0     1     0     0     0     0\n",
       "    0     1     0     0     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 4x10]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot = torch.FloatTensor(batch_size, 10)\n",
    "labels_onehot.zero_()\n",
    "labels_onehot.scatter_(1, labels.view(4, 1), 1)\n",
    "labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    out = torch.zeros(labels.size(0), num_classes)\n",
    "    out.scatter_(1, labels, 1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     1     0     0\n",
       "    0     0     0     0     0     1     0     0     0     0\n",
       "    0     1     0     0     0     0     0     0     0     0\n",
       "[torch.FloatTensor of size 4x10]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(labels.view(4, 1), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAACHCAYAAAAC53JtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgmjwi4ikjD/DxGgSHZYp5JnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8500de9f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the first batch\n",
    "out = torchvision.utils.make_grid(images)\n",
    "\n",
    "mean = np.array([0.1307])\n",
    "std = np.array([0.3081])\n",
    "\n",
    "imshow(out, title=[x for x in labels], mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 0.7608\n",
      "Epoch: [1/5], Step: [200/600], Loss: 0.7951\n",
      "Epoch: [1/5], Step: [300/600], Loss: 0.6894\n",
      "Epoch: [1/5], Step: [400/600], Loss: 0.7069\n",
      "Epoch: [1/5], Step: [500/600], Loss: 0.6831\n",
      "Epoch: [1/5], Step: [600/600], Loss: 0.5926\n",
      "Epoch: [2/5], Step: [100/600], Loss: 0.5411\n",
      "Epoch: [2/5], Step: [200/600], Loss: 0.6031\n",
      "Epoch: [2/5], Step: [300/600], Loss: 0.6395\n",
      "Epoch: [2/5], Step: [400/600], Loss: 0.4704\n",
      "Epoch: [2/5], Step: [500/600], Loss: 0.5185\n",
      "Epoch: [2/5], Step: [600/600], Loss: 0.6638\n",
      "Epoch: [3/5], Step: [100/600], Loss: 0.4431\n",
      "Epoch: [3/5], Step: [200/600], Loss: 0.5194\n",
      "Epoch: [3/5], Step: [300/600], Loss: 0.4523\n",
      "Epoch: [3/5], Step: [400/600], Loss: 0.5012\n",
      "Epoch: [3/5], Step: [500/600], Loss: 0.4002\n",
      "Epoch: [3/5], Step: [600/600], Loss: 0.4196\n",
      "Epoch: [4/5], Step: [100/600], Loss: 0.4130\n",
      "Epoch: [4/5], Step: [200/600], Loss: 0.3948\n",
      "Epoch: [4/5], Step: [300/600], Loss: 0.2985\n",
      "Epoch: [4/5], Step: [400/600], Loss: 0.4532\n",
      "Epoch: [4/5], Step: [500/600], Loss: 0.3540\n",
      "Epoch: [4/5], Step: [600/600], Loss: 0.5017\n",
      "Epoch: [5/5], Step: [100/600], Loss: 0.3986\n",
      "Epoch: [5/5], Step: [200/600], Loss: 0.4708\n",
      "Epoch: [5/5], Step: [300/600], Loss: 0.4074\n",
      "Epoch: [5/5], Step: [400/600], Loss: 0.4263\n",
      "Epoch: [5/5], Step: [500/600], Loss: 0.3712\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.4443\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # convert labels to one-hot\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                   % (epoch+1, num_epoches, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
